from fastapi import UploadFile
from docx import Document
import io
import requests
from bs4 import BeautifulSoup
import pdfplumber  # âœ… æ¨èä½¿ç”¨è¿™ä¸ªå¤„ç† PDF

# ğŸ‘‡ æ ¸å¿ƒæ”¹åŠ¨ï¼šæŠŠ pytesseract æ¢æˆ rapidocr
# å°è¯•å¯¼å…¥ RapidOCR (Python æœ€å¥½ç”¨çš„å¼€æº OCRï¼Œæ— éœ€å®‰è£…ç³»ç»Ÿè½¯ä»¶)
try:
    from rapidocr_onnxruntime import RapidOCR
    # åˆå§‹åŒ–å¼•æ“ (ç¬¬ä¸€æ¬¡è¿è¡Œä¼šè‡ªåŠ¨ä¸‹è½½æ¨¡å‹ï¼Œçº¦ 10MB)
    ocr_engine = RapidOCR()
    OCR_AVAILABLE = True
    print("âœ… [OCR] RapidOCR å¼•æ“åŠ è½½æˆåŠŸï¼(å·²å¯ç”¨ä¸­æ–‡å¢å¼ºè¯†åˆ«)")
except ImportError:
    OCR_AVAILABLE = False
    print("âš ï¸ æœªæ£€æµ‹åˆ° rapidocr_onnxruntimeï¼Œè¯·åœ¨ç»ˆç«¯è¿è¡Œ: pip install rapidocr_onnxruntime")

async def read_file_content(file: UploadFile) -> str:
    content = ""
    filename = file.filename.lower()
    
    print(f"ğŸ“‚ æ­£åœ¨è§£ææ–‡ä»¶: {filename}")

    try:
        file_bytes = await file.read()
        file_stream = io.BytesIO(file_bytes)

        # 1. å¤„ç† PDF (âœ… ä¼˜åŒ–ï¼šæ”¹ç”¨ pdfplumberï¼Œä¸­æ–‡æ•ˆæœæ›´å¥½)
        if filename.endswith(".pdf"):
            # pdfplumber éœ€è¦è¯»å–æµ
            with pdfplumber.open(file_stream) as pdf:
                for page in pdf.pages:
                    text = page.extract_text()
                    if text:
                        content += text + "\n"
        
        # 2. å¤„ç† Word æ–‡æ¡£
        elif filename.endswith(".docx"):
            doc = Document(file_stream)
            for para in doc.paragraphs:
                content += para.text + "\n"
        
        # 3. å¤„ç†å›¾ç‰‡ (RapidOCR)
        elif filename.endswith(('.png', '.jpg', '.jpeg', '.webp')):
            if OCR_AVAILABLE:
                try:
                    # RapidOCR å¯ä»¥ç›´æ¥æ¥æ”¶äºŒè¿›åˆ¶æ•°æ® (bytes)
                    result, _ = ocr_engine(file_bytes)
                    
                    if result:
                        # result çš„æ ¼å¼æ˜¯ [[åæ ‡], æ–‡æœ¬, ç½®ä¿¡åº¦]
                        text_list = [item[1] for item in result]
                        content = "\n".join(text_list)
                        print(f"âœ… å›¾ç‰‡è¯†åˆ«æˆåŠŸï¼Œå…±æå– {len(content)} ä¸ªå­—ç¬¦")
                    else:
                        content = "ã€æœªèƒ½è¯†åˆ«å‡ºæ–‡å­—ï¼Œå›¾ç‰‡å¯èƒ½å¤ªæ¨¡ç³Šæˆ–æ²¡æœ‰æ–‡å­—ã€‘"
                except Exception as e:
                    print(f"âŒ OCR è¯†åˆ«è¿‡ç¨‹å‡ºé”™: {e}")
                    content = "ã€å›¾ç‰‡å†…å®¹è¯†åˆ«ç³»ç»Ÿå‡ºé”™ã€‘"
            else:
                content = "ã€ç³»ç»Ÿæœªå®‰è£… RapidOCR åº“ï¼Œæ— æ³•è§£æå›¾ç‰‡ï¼Œè¯·æ£€æŸ¥ pip å®‰è£…ã€‘"

        # 4. å…¶ä»–æ–‡ä»¶å½“çº¯æ–‡æœ¬å¤„ç†
        else:
            try:
                content = file_bytes.decode("utf-8")
            except:
                content = file_bytes.decode("gbk", errors="ignore")

    except Exception as e:
        print(f"âŒ æ–‡ä»¶è§£æå‡ºé”™: {e}")
        return None
    
    # âŒ ç§»é™¤ return content[:5000] é™åˆ¶ï¼Œç¡®ä¿è¿”å›å…¨æ–‡
    return content

def read_url_content(url: str) -> str:
    # é’ˆå¯¹å°çº¢ä¹¦é“¾æ¥çš„ç‰¹æ®Šæç¤º
    if "xiaohongshu" in url:
        return f"æ£€æµ‹åˆ°å°çº¢ä¹¦é“¾æ¥ï¼š{url}\nç”±äºå°çº¢ä¹¦åçˆ¬ä¸¥æ ¼ï¼Œå»ºè®®æ‚¨ç›´æ¥ã€æˆªå›¾ã€‘å¹¶ä½¿ç”¨å›¾ç‰‡ä¸Šä¼ åŠŸèƒ½ï¼Œæˆ–ç›´æ¥å¤åˆ¶æ–‡å­—å†…å®¹ç²˜è´´ã€‚"

    # å…¶ä»–ç½‘é¡µçš„é€šç”¨çˆ¬å–
    print(f"ğŸ”— [Crawler] æ­£åœ¨æŠ“å–: {url}")
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        }
        response = requests.get(url, headers=headers, timeout=10)
        soup = BeautifulSoup(response.text, 'html.parser')
        
        for tag in soup(["script", "style", "nav", "footer"]):
            tag.extract()
            
        text = soup.get_text()
        # âŒ ç§»é™¤æˆªæ–­ï¼Œè¿”å›å…¨æ–‡
        return text.strip()
    except Exception as e:
        return f"æ— æ³•æŠ“å–è¯¥ç½‘é¡µï¼Œå»ºè®®å¤åˆ¶å†…å®¹ä¸Šä¼ ã€‚é“¾æ¥: {url}"