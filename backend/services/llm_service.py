import os
import json
from pathlib import Path
from openai import OpenAI
from dotenv import load_dotenv

# ================= ç¯å¢ƒé…ç½®ä¸åˆå§‹åŒ– =================
current_file_path = Path(__file__).resolve()
project_root = current_file_path.parent.parent
env_path = project_root / ".env"

print(f"ğŸ” [Debug] æ­£åœ¨åŠ è½½é…ç½®æ–‡ä»¶: {env_path}")
load_dotenv(dotenv_path=env_path)

# ğŸ‘‡ æ ¸å¿ƒåˆ‡æ¢ï¼šè¯»å–ä¸ƒç‰›é…ç½®
api_key = os.getenv("QINIU_API_KEY")
base_url = os.getenv("QINIU_BASE_URL", "https://ap-gate-z0.qiniuapi.com/v1")
MODEL_NAME = os.getenv("QINIU_MODEL_NAME", "deepseek/deepseek-v3.2-251201")

if not api_key:
    #print("âŒ [Fatal] æœªæ‰¾åˆ° QINIU_API_KEYï¼Œè¯·æ£€æŸ¥ .env æ–‡ä»¶ï¼")
    final_api_key = "MISSING"
else:
    final_api_key = api_key
    #print(f"âœ… [Success] ä¸ƒç‰› API Key åŠ è½½æˆåŠŸ (é•¿åº¦: {len(api_key)})")

# åˆå§‹åŒ–å®¢æˆ·ç«¯ (å¸¦è¶…æ—¶ä¿æŠ¤)
client = OpenAI(
    api_key=final_api_key,
    base_url=base_url,
    timeout=60.0, # å…¨å±€ 60ç§’è¶…æ—¶ï¼Œé˜²æ­¢æ— é™ç­‰å¾…
)

print(f"ğŸ”Œ [LLM Service] å·²è¿æ¥: {MODEL_NAME}")


# =================  æ ¸å¿ƒ PROMPTS =================

# 1. ä¸Šä¼ åˆ†ç±» Prompt (åŒ…å« themes å’Œ tags)
PROMPT_UPLOAD = """
# è§’è‰²
ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„ä½œæ–‡ç´ ææ¶æ„å¸ˆã€‚ä½ çš„ä»»åŠ¡æ˜¯å°†ç”¨æˆ·è¾“å…¥çš„éç»“æ„åŒ–æ–‡æœ¬{{input}}è¿›è¡Œæ¸…æ´—ã€æ‹†è§£ï¼Œå¹¶è½¬åŒ–ä¸ºç»“æ„åŒ–çš„ç´ æå¡ç‰‡åˆ—è¡¨ã€‚

# æ ¸å¿ƒåŸåˆ™ (å¿…é¡»éµå®ˆ)
**ä¸¥ç¦è„‘è¡¥**ï¼šç»å¯¹ä¸è¦è¡¥å…¨åŸæ–‡ä¸­çœ‹ä¼¼ç¼ºå¤±çš„è¯è¯­ã€‚å¦‚æœåŸæ–‡æ˜¯ "å®¡è§†XXçš„ç»´åº¦"ï¼Œä½ å¿…é¡»ä¿ç•™ "XX"ï¼Œç»ä¸èƒ½æ”¹æˆ "å®¡è§†é—®é¢˜çš„ç»´åº¦"ã€‚

## æŠ€èƒ½ 1: æ™ºèƒ½æ‹†åˆ† (Smart Split)
- **å¤šç´ æè¯†åˆ«**ï¼šå¦‚æœè¾“å…¥åŒ…å«å¤šä¸ªç‹¬ç«‹çš„äººç‰©æ•…äº‹ã€å¤šå¥ä¸ç›¸å…³çš„åè¨€ã€æˆ–å¤šä¸ªæ˜æ˜¾çš„è®ºè¯æ®µè½ï¼Œè¯·åŠ¡å¿…å°†å®ƒä»¬**æ‹†åˆ†**ä¸ºå¤šä¸ªç‹¬ç«‹çš„ç´ æå¯¹è±¡ã€‚
- **å•ç´ æä¿æŒ**ï¼šå¦‚æœè¾“å…¥æ˜¯ä¸€ç¯‡è¿è´¯çš„æ–‡ç« æˆ–ä¸€ä¸ªå®Œæ•´çš„æ•…äº‹ï¼Œåˆ™ä½œä¸ºä¸€ä¸ªç´ æå¤„ç†ã€‚

## æŠ€èƒ½ 2: æ·±åº¦æ¸…æ´— (Deep Cleaning)
- **å»å™ª**ï¼šå»é™¤â€œç‚¹å‡»å…³æ³¨â€ã€â€œå¹¿å‘Šâ€ã€â€œå°ç¼–è¯´â€ã€â€œæ¥æºç½‘ç»œâ€ã€â€œé¡µç â€ç­‰æ— ç”¨ä¿¡æ¯ã€‚
- **ä¿®å¤**ï¼šä¿®æ­£ OCR å¯¼è‡´çš„é”™åˆ«å­—æˆ–æ–­å¥ã€‚

## æŠ€èƒ½ 3: æ ‡å‡†åŒ–å½’ç±» (Standardization)
1. **ç±»å‹ (type)**ï¼šåªèƒ½ä»ä»¥ä¸‹åˆ—è¡¨ä¸­é€‰æ‹© 1 ä¸ªï¼š
   ["äººç‰©ç´ æ", "åè¨€é‡‘å¥", "è®ºè¯æ®µ", "å¼€å¤´æ®µ", "ç»“å°¾æ®µ", "ä¸“ä¸šè¯æ±‡", "èŒƒæ–‡"]

2. **æ ¸å¿ƒä¸»é¢˜ (themes)**ï¼šä»ä»¥ä¸‹ 18 ä¸ªæ ¸å¿ƒä¸»é¢˜ä¸­ï¼Œé€‰æ‹© **1-3 ä¸ª**æœ€è´´åˆçš„ä¸»é¢˜ï¼š
   ["é’æ˜¥å¥‹æ–—", "å®¶å›½æƒ…æ€€", "ç§‘æŠ€åˆ›æ–°", "è´£ä»»å¥‰çŒ®", "è‹¦éš¾æŒ«æŠ˜", "æ–‡åŒ–ä¼ æ‰¿", "æ¦œæ ·åŠ›é‡", "å…¬å¹³æ­£ä¹‰", "ç”Ÿæ€ç¯ä¿", "å¤šå…ƒåŒ…å®¹", "äººæ€§å…‰è¾‰", "ç½‘ç»œæ—¶ä»£", "è‡ªæˆ‘è®¤çŸ¥", "äººç”Ÿç†æƒ³", "å·¥åŒ ç²¾ç¥", "æ–‡åŒ–è‡ªä¿¡", "è´£ä»»æ‹…å½“", "å®¡ç¾å¢ƒç•Œ"]

3. **æ™ºèƒ½æ ‡ç­¾ (tags)**ï¼šåŸºäºå†…å®¹ç”Ÿæˆ **0-5 ä¸ªå…·ä½“çš„å…³é”®è¯æ ‡ç­¾**ï¼Œç”¨äºè¡¥å……æ ¸å¿ƒä¸»é¢˜ä¹‹å¤–çš„ä¿¡æ¯ï¼ˆå¦‚å…·ä½“äººç‰©åã€ä¿®è¾æ‰‹æ³•ã€æƒ…æ„ŸåŸºè°ƒç­‰ï¼‰ã€‚
   - ä¾‹å¦‚ï¼šâ€œæç™½â€ã€â€œæ¯”å–»è®ºè¯â€ã€â€œç»†èŠ‚æå†™â€ã€â€œä¹è§‚è±è¾¾â€ã€‚

## é™åˆ¶
- **å¿…é¡»è¾“å‡ºæ ‡å‡† JSON æ•°ç»„æ ¼å¼** `[...]`ã€‚
- JSON ç»“æ„ç¤ºä¾‹ï¼š
[
  {
    "type": "äººç‰©ç´ æ",
    "themes": ["å®¶å›½æƒ…æ€€", "è‹¦éš¾æŒ«æŠ˜"], 
    "tags": ["è‹è½¼", "é»„å·çªå›´", "ä¹è§‚å¿ƒæ€"],
    "content": "å†…å®¹..."
  }
]
"""

# 2. è§£æç‚¹è¯„ Prompt
PROMPT_ANALYZE = """
# è§’è‰²
ä½ æ˜¯ä¸€ä½ç²¾é€šé«˜è€ƒä½œæ–‡è¯„åˆ†æ ‡å‡†çš„ä¸“ä¸šé«˜ä¸­è¯­æ–‡è€å¸ˆã€‚

## æŠ€èƒ½
### æŠ€èƒ½1ï¼šä½œæ–‡ç´ æç‚¹è¯„
1. **åˆ†æç´ æç±»å‹**ï¼šåˆ¤æ–­ç´ æå±äºè®°å™æ–‡ã€è®®è®ºæ–‡ã€æ•£æ–‡æˆ–ç»¼åˆç±»ã€‚
2. **å†…å®¹è¯„ä»·**ï¼šåˆ†æä¼˜ç‚¹ï¼ˆå†…å®¹å……å®ã€ç«‹æ„æ·±åˆ»ï¼‰å’Œä¸è¶³ã€‚
3. **é€‚ç”¨æ–‡ä½“**ï¼šç‚¹æ˜ç´ ææœ€é€‚é…çš„ä½œæ–‡ç±»å‹å¹¶è¯´æ˜åŸå› ã€‚

### æŠ€èƒ½2ï¼šå†™ä½œè§’åº¦æ‹†è§£
1. **å¤šç»´åº¦åˆ†æ**ï¼šä»â€œä¸ªäººæˆé•¿/ç¤¾ä¼šç°è±¡/æ–‡åŒ–ä¼ æ‰¿/æ—¶ä»£ç²¾ç¥/æ€è¾¨å…³ç³»â€ç»´åº¦åˆ‡å…¥ã€‚
2. **è§’åº¦å…·è±¡åŒ–**ï¼šæ‹†è§£ä¸ªäººè§†è§’ã€æ€è¾¨è§†è§’æˆ–ç¤¾ä¼šè§†è§’ã€‚
3. **è®ºè¯é€»è¾‘æç¤º**ï¼šè¯´æ˜æ¯ä¸ªè§’åº¦çš„ç´ ææ”¯æ’‘ç‚¹ã€‚

### æŠ€èƒ½3ï¼šé€‚ç”¨ä¸»é¢˜æ¨è
1. **ç›´æ¥/é—´æ¥é€‚ç”¨ä¸»é¢˜**ï¼šæ¨èé«˜è€ƒé«˜é¢‘ä¸»é¢˜ã€‚
2. **åº”ç”¨åœºæ™¯ç¤ºä¾‹**ï¼šæä¾›å…·ä½“ä½œæ–‡é¢˜çš„ç´ æåµŒå…¥æ–¹æ³•ã€‚

## é™åˆ¶
- **å­—æ•°æ§åˆ¶ä¸å®Œæ•´æ€§**ï¼šåœ¨å­—æ•°å…è®¸èŒƒå›´å†…ï¼ˆå„éƒ¨åˆ†200-500å­—ï¼‰ï¼Œå¿…é¡»ç¡®ä¿è¯­å¥é€šé¡ºã€é€»è¾‘å®Œæ•´ï¼Œä¸¥ç¦å› å­—æ•°é™åˆ¶è€Œç”Ÿç¡¬åˆ‡æ–­ã€‚
- è¾“å‡ºæ ¼å¼ä¸¥æ ¼ä½¿ç”¨ã€ç®€è¯„ã€‘ã€å†™ä½œè§’åº¦ã€‘ã€é€‚ç”¨ä¸»é¢˜ã€‘ä¸‰ä¸ªæ¨¡å—ã€‚
"""

# 3. ä»¿å†™å‡ºé¢˜ Prompt
PROMPT_IMITATE = """
# è§’è‰²
ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„ä½œæ–‡ä»¿å†™æŒ‡å¯¼è€å¸ˆã€‚

## å‰ç½®æ ¡éªŒ (Pre-check)
åœ¨æ‰§è¡ŒæŠ€èƒ½å‰ï¼Œè¯·å…ˆæ£€æµ‹ç”¨æˆ·è¾“å…¥çš„{{input}}æ˜¯å¦å…·å¤‡â€œèŒƒæ–‡â€çš„åŸºæœ¬å®Œæ•´æ€§ï¼š
1. **å­—æ•°æ£€æµ‹**ï¼šè‹¥è¾“å…¥å†…å®¹è¿‡çŸ­ï¼ˆä¾‹å¦‚å°‘äº50å­—ï¼‰ï¼Œä¸è¶³ä»¥æ„æˆæ®µè½æˆ–ç¯‡ç« ã€‚
2. **é€»è¾‘æ£€æµ‹**ï¼šè‹¥è¾“å…¥å†…å®¹ç¼ºä¹åŸºæœ¬çš„é€»è¾‘ç»“æ„ã€‚
**è‹¥æ»¡è¶³ä»¥ä¸Šä»»ä¸€æ¡ä»¶ï¼Œè¯·ç›´æ¥å›å¤ï¼šâ€œæ£€æµ‹åˆ°æ‚¨è¾“å…¥çš„å†…å®¹è¿‡çŸ­æˆ–é€»è¾‘ä¸å…¨ï¼Œéš¾ä»¥è¿›è¡Œæœ‰æ•ˆçš„ä»¿å†™æ‹†è§£ã€‚è¯·ä¸Šä¼ å®Œæ•´çš„æ®µè½æˆ–èŒƒæ–‡ã€‚â€**

## æŠ€èƒ½
### æŠ€èƒ½1ï¼šèŒƒæ–‡æ‹†è§£åˆ†æ
1. **ä¸»æ—¨æç‚¼**ï¼šæ€»ç»“æ ¸å¿ƒä¸»é¢˜ã€‚
2. **æ¡†æ¶è§£æ„**ï¼šæ‹†åˆ†ç»“æ„ã€‚
3. **ç»†èŠ‚æ‹†è§£**ï¼šè¯†åˆ«è®ºè¯æ–¹å¼å’Œè¯­è¨€ç‰¹è‰²ã€‚

### æŠ€èƒ½2ï¼šä½œæ–‡é¢˜ç›®è®¾è®¡
è®¾è®¡3ç±»ä»¿å†™é¢˜ç›®ï¼Œè¦æ±‚ä¸»é¢˜å…³è”ã€ç»“æ„åŒ¹é…ã€‚

### æŠ€èƒ½3ï¼šæ®µè½ä»¿å†™é¢˜ç”Ÿæˆ
é’ˆå¯¹ç»å…¸æ®µè½è®¾è®¡ä»¿å†™ä»»åŠ¡ï¼ˆç»“æ„æ¨¡ä»¿ã€æ‰‹æ³•è¿ç§»ï¼‰ã€‚

## é™åˆ¶
- è¾“å‡ºæ—¶ä¼˜å…ˆåˆ†ç‚¹ï¼ˆæ ‡é¢˜+æ­£æ–‡ï¼‰ã€‚
"""

# ================= åŠŸèƒ½å‡½æ•° =================

def service_process_upload(text_content):
    print(f"ğŸ¤– [LLM Request] Model: {MODEL_NAME}, Content Length: {len(text_content)}")
    try:
        # å‘é€è¯·æ±‚ (æ—  response_formatï¼Œå…¼å®¹æ€§æœ€å¥½)
        response = client.chat.completions.create(
            model=MODEL_NAME,
            messages=[
                {"role": "system", "content": PROMPT_UPLOAD},
                {"role": "user", "content": text_content}
            ],
            temperature=0.2
        )
        result_str = response.choices[0].message.content
        print(f"ğŸ“© [LLM Response Received]: {len(result_str)} chars") 
        
        # === ğŸ›¡ï¸ å¼ºåŠ›æ¸…æ´—é€»è¾‘ (é˜²æ­¢ AI è¯´åºŸè¯) ===
        clean_str = result_str.strip()
        if "```" in clean_str:
            clean_str = clean_str.replace("```json", "").replace("```", "").strip()
        
        # å°è¯•æå– [ ... ]
        start_idx = clean_str.find("[")
        end_idx = clean_str.rfind("]")
        if start_idx != -1 and end_idx != -1:
            clean_str = clean_str[start_idx : end_idx+1]
            
        try:
            data = json.loads(clean_str)
        except json.JSONDecodeError:
            print(f"âš ï¸ JSON è§£æå¤±è´¥ï¼Œå°è¯•ä¿®å¤: {clean_str[:50]}...")
            return [{
                "type": "æœªåˆ†ç±»", "themes": [], "tags": ["æ ¼å¼é”™è¯¯"],
                "content": text_content
            }]
        
        if isinstance(data, list): return data
        if isinstance(data, dict):
            if "materials" in data and isinstance(data["materials"], list): return data["materials"]
            return [data]
        return []

    except Exception as e:
        print(f"âŒ Upload Error: {e}")
        return [{
            "type": "æœªåˆ†ç±»",
            "themes": [], 
            "tags": ["AIæœåŠ¡å¼‚å¸¸"], 
            "content": f"AI è¿æ¥é”™è¯¯: {str(e)}ã€‚åŸå§‹å†…å®¹: {text_content[:50]}..."
        }]

def service_analyze_material(material_content):
    try:
        response = client.chat.completions.create(
            model=MODEL_NAME,
            messages=[{"role": "system", "content": PROMPT_ANALYZE}, {"role": "user", "content": material_content}],
            temperature=0.7
        )
        return response.choices[0].message.content
    except Exception as e:
        return f"è§£ææœåŠ¡æš‚æ—¶ä¸å¯ç”¨: {str(e)}"

def service_generate_imitation(sample_text):
    try:
        response = client.chat.completions.create(
            model=MODEL_NAME,
            messages=[{"role": "system", "content": PROMPT_IMITATE}, {"role": "user", "content": sample_text}],
            temperature=0.8
        )
        return response.choices[0].message.content
    except Exception as e:
        return f"ä»¿å†™æœåŠ¡æš‚æ—¶ä¸å¯ç”¨: {str(e)}"

def service_chat(user_message, system_prompt=None):
    try:
        msgs = [{"role": "user", "content": user_message}]
        if system_prompt:
            msgs.insert(0, {"role": "system", "content": system_prompt})
            
        response = client.chat.completions.create(
            model=MODEL_NAME,
            messages=msgs,
            temperature=0.7
        )
        return response.choices[0].message.content
    except Exception as e:
        return f"å¯¹è¯æœåŠ¡æš‚æ—¶ä¸å¯ç”¨: {str(e)}"